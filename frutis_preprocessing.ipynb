{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "368a09ab",
   "metadata": {},
   "source": [
    "# Projet 8 : Images preprocessing spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfdf196-e5ad-4dff-8cc9-ae1b505ff0f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Uploading Database on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08400221-1834-4711-9ab7-8fbe1094ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade --user awscli   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6032ef-81e5-4fc4-a0bf-9d43e4aa9dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source = \"/mnt/d/Openclassrooms/Projets/P8_KOKORA_AMON/archive/fruits-360-original-size/fruits-360-original-size/Training/\"\n",
    "# destination  = \"s3://jpkokora/ocr_p08/Training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549ee443-5b2f-4b77-b292-bb6ba540aa8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !aws s3 sync source destination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba1ef76",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Libraries and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afb44a8-0631-475f-819c-7f030c03e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9cfc5d-02c9-44ce-870c-6c3c318d852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.ml.linalg import Vectors\n",
    "# from pyspark.sql.functions import *\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1162aa8-7799-45a4-9db2-d36bd4c07f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblibspark import register_spark\n",
    "# from sklearn.utils import parallel_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d32f50d6-cb22-4713-9041-bcfeee29e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Image preprcocessing\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40e626b-5c1a-4568-a836-6b262e0036c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.21.60.170:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Image preprcocessing</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6154aa6730>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fe2e106-c39a-4be6-b904-a96152e923a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data \n",
    "path=\"/mnt/d/Openclassrooms/Projets/P8_KOKORA_AMON/archive/fruits-360-original-size/fruits-360-original-size/Training/apple_6/\"\n",
    "#path = \"s3://jpkokora/ocr_p8/Training/**\"\n",
    "# images = spark.read.format(\"image\").load(path)\n",
    "# mysplit = udf(lambda z: z.split('/')[-2]) \n",
    "# images = images.withColumn('label',mysplit(col('image.origin')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4bb0562-0fe5-4589-a91e-990be60dbeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f3310b0-dba8-4c9a-b981-d2e19dd61b70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /mnt/d/model_mobile_v2/assets\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, smart_resize\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "base_model_mn2 = MobileNetV2(weights=\"imagenet\", input_shape=(224, 224, 3))\n",
    "# remove the output layer\n",
    "my_mn2 = Model(inputs=base_model_mn2.inputs, outputs=base_model_mn2.layers[-2].output)\n",
    "\n",
    "my_mn2.save('/mnt/d/model_mobile_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab215bc9-4775-48d7-9ac4-724dc387d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_learning(iterator):\n",
    "    from tensorflow.keras.models import \n",
    "    # model = load_model(\"s3://jpkokora/ocr_p8/model_mobilenet2.h5\")\n",
    "    model = load_model(\"/mnt/d/model_mobile_v2\")\n",
    "    for record in iterator:\n",
    "        yield model.predict(record)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "433704f3-ad0e-45e5-a091-4df7becd8024",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def features_extractor(data, spark_obj):\n",
    "    from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql.functions import udf, col\n",
    "    from pyspark.ml.image import ImageSchema\n",
    "    from tensorflow.keras.applications import MobileNetV2\n",
    "    from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "    from tensorflow.keras.preprocessing.image import load_img, img_to_array, smart_resize\n",
    "    \n",
    "    images = spark_obj.read.format(\"image\").load(path)\n",
    "    mysplit = udf(lambda z: z.split('/')[-2]) \n",
    "    images = images.withColumn('label',mysplit(col('image.origin')))\n",
    "    labels = images.select('label')\n",
    "    data_rdd = images.rdd\n",
    "    #libraries\n",
    "    \n",
    "    ImageSchema.toNDArray(data_rdd.first()['image'])\n",
    "\n",
    "    data_rdd = data_rdd.map(lambda x : ImageSchema.toNDArray(x['image'])) \\\n",
    "        .map(lambda x : smart_resize(x, size=(224,224))) \\\n",
    "        .map(lambda x: x.reshape((1, x.shape[0], x.shape[1], x.shape[2]))) \\\n",
    "        .map(lambda x: preprocess_input(x)) \\\n",
    "        .mapPartitions(transfer_learning) \n",
    "    \n",
    "    data_rdd = spark_obj.createDataFrame(data_rdd)\n",
    "    # since there is no common column between these two dataframes add row_index so that it can be joined\n",
    "    data_rdd=data_rdd.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "    labels=labels.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "\n",
    "    data_rdd = labels.join(data_rdd, on=[\"row_index\"]).drop(\"row_index\")\n",
    "\n",
    "    return data_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb6e5b06-84d5-493d-97a1-fd4724968f20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = features_extractor(path, spark).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aae3be28-30a4-488d-922f-8b0dcfeda2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "747a6510-1365-48da-9e05-28f85527514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "  inputCols=data.columns[1:], outputCol=\"pfeatures\", \n",
    ")\n",
    "scaler = StandardScaler(inputCol=\"pfeatures\", outputCol=\"scaledFeatures\",\n",
    "                         withStd=True, withMean=True)\n",
    "pca = PCA(k=850, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "988781b5-5963-4e21-a5db-5d567aa17db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[assembler,scaler,pca]).fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57662a1f-9ab1-4e5b-b207-b84c46ace111",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pipeline.transform(data).select('label','pcaFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43f0cb40-83c8-4d08-b4cc-fa88330b1567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: string (nullable = true)\n",
      " |-- pcaFeatures: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(pyspark.sql.dataframe.DataFrame, None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data), data.printSchema(), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa50f35c-66ca-4cb9-a6ee-af756ae78863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(row):\n",
    "    return (row.label, ) + tuple(row.pcaFeatures.toArray().tolist())\n",
    "\n",
    "data = data.rdd.map(extract).toDF([\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1e11282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving data\n",
    "# import os\n",
    "data.write.option(\"header\",\"true\").csv(\"/mnt/d/csv/data_cleaned\",mode=\"overwrite\")\n",
    "# data.write.option(\"header\",\"true\").csv(\"s3://jpkokora/ocr_p8/data_preproceed\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf6f0508-2ffb-4d36-a93d-084f54c32cfc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/mnt/d/csv/data_cleaned.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m re \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/mnt/d/csv/data_cleaned.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:158\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/mnt/d/csv/data_cleaned.csv"
     ]
    }
   ],
   "source": [
    "re = spark.read.format('csv').options(header='true', inferSchema='true').load(\"/mnt/d/csv/data_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02288d0d-7cf1-4c57-9c86-f522a3957259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
